{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9698f9a2-76e1-48dd-a217-0565f3a997b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from osgeo import gdal, ogr\n",
    "import errno\n",
    "import argparse\n",
    "import itertools\n",
    "import glob\n",
    "from subprocess import call\n",
    "from collections import defaultdict\n",
    "import json\n",
    "from datetime import timedelta\n",
    "from datetime import datetime as dt\n",
    "import shapely.wkt\n",
    "from shapely.geometry import Polygon, box\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import boto3\n",
    "from botocore import UNSIGNED\n",
    "from botocore.config import Config\n",
    "from multiprocessing import Pool, cpu_count\n",
    "from itertools import repeat\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "s3 = boto3.resource('s3', config=Config(signature_version=UNSIGNED))\n",
    "s3_client = boto3.client('s3')\n",
    "gdal.SetConfigOption('AWS_NO_SIGN_REQUEST', 'YES')\n",
    "\n",
    "import pandas as pd\n",
    "from rasterstats import zonal_stats\n",
    "import geopandas as gpd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "06d58633-b907-4c91-9ae8-cf56ed38ebf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mgrs_shp(aoi_shp, bbox):\n",
    "    aoi_mgrs_shp = 'sentinel2_bbox_mgrs.shp'\n",
    "    try:\n",
    "        os.remove(aoi_mgrs_shp)\n",
    "    except OSError:\n",
    "        pass\n",
    "    if aoi_shp:\n",
    "        call(str('ogr2ogr -clipsrc ' + aoi_shp + ' ' + aoi_mgrs_shp + ' ' + s2_tile), shell=True)\n",
    "    else:\n",
    "        min_x, min_y, max_x, max_y = bbox[0], bbox[1], bbox[2], bbox[3]\n",
    "        call(str('ogr2ogr -f \"ESRI Shapefile\" ' + aoi_mgrs_shp + ' ' + s2_tile + ' -clipsrc ' + str(min_x)\n",
    "                 + ' ' + str(min_y) + ' ' + str(max_x) + ' ' + str(max_y)), shell=True)\n",
    "    return aoi_mgrs_shp\n",
    "\n",
    "\n",
    "def shape_to_tiles(aoi_shp=None, bbox=None):\n",
    "    aoi_mgrs_shp = get_mgrs_shp(aoi_shp, bbox)\n",
    "    driver = ogr.GetDriverByName('ESRI Shapefile')\n",
    "    ds = driver.Open(aoi_mgrs_shp)\n",
    "    layer = ds.GetLayer(0)\n",
    "    mgrs_list = []\n",
    "    for feat in layer:\n",
    "        mgrs_list.append(feat.GetField('tile'))\n",
    "    del layer\n",
    "    del ds\n",
    "    return mgrs_list\n",
    "\n",
    "def shape_to_polygon(shp_file=None, bbox=None):\n",
    "    poly = None\n",
    "    if bbox:\n",
    "        poly = box(*bbox, ccw=True)\n",
    "    elif shp_file:\n",
    "        ds = ogr.Open(shp_file)\n",
    "        layer = ds.GetLayer(0)\n",
    "        for feat in layer:\n",
    "            wkt_poly = feat.geometry().ExportToWkt()\n",
    "        poly = shapely.wkt.loads(wkt_poly)\n",
    "    return poly\n",
    "\n",
    "\n",
    "def dates_dif(date1, date2):\n",
    "    d0 = dt.strptime(date1, '%Y-%m-%d').date()\n",
    "    d1 = dt.strptime(date2, '%Y-%m-%d').date()\n",
    "    delta = abs(d1 - d0).days\n",
    "    return delta\n",
    "\n",
    "\n",
    "def validate_date(original_date, start_date, end_date):\n",
    "    final_date = dt.strptime(str(original_date), '%Y-%m-%d')\n",
    "    s_date = dt.strptime(str(start_date), '%Y-%m-%d')\n",
    "    e_date = dt.strptime(str(end_date), '%Y-%m-%d')\n",
    "    return s_date <= final_date <= e_date\n",
    "\n",
    "\n",
    "def datetime_iterator(start_date=None, end_date=None):\n",
    "    if not end_date:\n",
    "        end_date = dt.today().date()\n",
    "    if not start_date:\n",
    "        start_date = end_date - timedelta(30)\n",
    "    start_date = dt.strptime(str(start_date), '%Y-%m-%d').date()\n",
    "    start_date = start_date.replace(day=1)\n",
    "    end_date = dt.strptime(str(end_date), '%Y-%m-%d').date()\n",
    "    while start_date <= end_date:\n",
    "        yield start_date\n",
    "        start_date = start_date + relativedelta(months=1)\n",
    "\n",
    "\n",
    "def data_difference_days(product_dictionary, days_interval):\n",
    "    from datetime import datetime as dt\n",
    "    prev_date = None\n",
    "    _dates = list(sorted(product_dictionary.keys()))\n",
    "    n = len(_dates)\n",
    "    for i in range(n - 1):\n",
    "        if not prev_date:\n",
    "            prev_date = _dates[i]\n",
    "        date = _dates[i + 1]\n",
    "        date_diff = dt.strptime(date, \"%Y-%m-%d\") - dt.strptime(prev_date, \"%Y-%m-%d\")\n",
    "        days_diff = date_diff.days\n",
    "        if days_diff < days_interval:\n",
    "            product_dictionary.pop(date)\n",
    "        else:\n",
    "            prev_date = None\n",
    "    return product_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "36e98597-6fc1-4e57-86ae-0c03f9a0d487",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_product_ids(start_date, end_date, cloud_threshold, data_days_interval, shape_file=None, bbox=None):\n",
    "    source_s3_bucket = 'sentinel-cogs'\n",
    "    source_s3_dir = 'sentinel-s2-l2a-cogs'\n",
    "    tile_list = shape_to_tiles(aoi_shp=shape_file, bbox=bbox)\n",
    "    tile_list = ['43RDM']\n",
    "    poly = shape_to_polygon(shp_file=shape_file, bbox=bbox)\n",
    "    print(f'Tiles found for the given AOI : {tile_list}')\n",
    "    final_dict = {'single_tile': {}, 'merge_tile': {}}\n",
    "    for tile in tile_list:\n",
    "        utm_zone, lat_band, grid_square = str(tile)[:2], str(tile)[2], str(tile)[3:5]\n",
    "        for _date in datetime_iterator(start_date, end_date):\n",
    "            _year = int(_date.year)\n",
    "            _month = int(_date.month)\n",
    "            _PREFIX = os.path.join(source_s3_dir, str(utm_zone), str(lat_band), str(grid_square),\n",
    "                                   str(_year), str(_month), '')\n",
    "            _PREFIX = _PREFIX.replace('\\\\', '/')\n",
    "            response = s3_client.list_objects(Bucket=source_s3_bucket, Prefix=_PREFIX)\n",
    "            for content in response.get('Contents', []):\n",
    "                key = content['Key']\n",
    "                if key.endswith('.json'):\n",
    "                    product_id = str(key).split('/')[-2]\n",
    "                    pid_date = '-'.join([str(_year), str(_month).zfill(2), str(product_id[16:18]).zfill(2)])\n",
    "                    if not validate_date(pid_date, start_date, end_date):\n",
    "                        continue\n",
    "                    result = s3.Object(source_s3_bucket, key)\n",
    "                    data = json.load(result.get()['Body'])\n",
    "                    tile_coord = data['geometry']['coordinates']\n",
    "                    tile_cloud = data['properties']['eo:cloud_cover']\n",
    "                    tile_poly = Polygon(tile_coord[0])\n",
    "                    percent_area = poly.intersection(tile_poly).area / poly.area\n",
    "                    print(f'Date: {pid_date}   Tile:{tile}   Cloud: {tile_cloud}   Area AOI/Tile: {percent_area}')\n",
    "                    if percent_area <= 0.05:\n",
    "                        continue\n",
    "                    elif percent_area >= 0.99:\n",
    "                        if not final_dict['single_tile'].get(str(pid_date)):\n",
    "                            final_dict['single_tile'][str(pid_date)] = {}\n",
    "                        final_dict['single_tile'][str(pid_date)][str('pids')] = [product_id]\n",
    "                        final_dict['single_tile'][str(pid_date)][str('cloud_percentages')] = tile_cloud\n",
    "                    else:\n",
    "                        if not final_dict['merge_tile'].get(str(pid_date)):\n",
    "                            final_dict['merge_tile'][str(pid_date)] = defaultdict(list)\n",
    "                        final_dict['merge_tile'][str(pid_date)][str('pids')].append(product_id)\n",
    "                        final_dict['merge_tile'][str(pid_date)][str('tile_ids')].append(tile)\n",
    "                        final_dict['merge_tile'][str(pid_date)][str('cloud_percentages')].append(tile_cloud)\n",
    "                        final_dict['merge_tile'][str(pid_date)][str('percent_areas')].append(percent_area)\n",
    "    diff = dates_dif(start_date, end_date)\n",
    "    diff = diff // data_days_interval\n",
    "    diff = diff // 2  #### Data interval Buffer for user's input\n",
    "    all_pids = {}\n",
    "    if len(final_dict['single_tile']) >= diff:\n",
    "        data = final_dict['single_tile']\n",
    "        _dates = list(sorted(data.keys()))\n",
    "        for date in _dates:\n",
    "            print(f'Single Tile Metadata: {data[date]}')\n",
    "            tile_cloud = data[date]['cloud_percentages']\n",
    "            if tile_cloud > cloud_threshold:\n",
    "                continue\n",
    "            else:\n",
    "                all_pids[str(date)] = data[date]['pids']\n",
    "    else:\n",
    "        data = final_dict['merge_tile']\n",
    "        prev_date = None\n",
    "        skip_one = False\n",
    "        _dates = list(sorted(data.keys()))\n",
    "        for date in _dates:\n",
    "            if not prev_date:\n",
    "                prev_date = date\n",
    "                continue\n",
    "            if skip_one:\n",
    "                prev_date = date\n",
    "                skip_one = False\n",
    "                continue\n",
    "            date_diff = dt.strptime(date, \"%Y-%m-%d\") - dt.strptime(prev_date, \"%Y-%m-%d\")\n",
    "            days_diff = date_diff.days\n",
    "            weighted_sum = 0\n",
    "            sum_area_percents = 0\n",
    "            for i in range(len(data[prev_date]['tile_ids'])):\n",
    "                weighted_sum += data[prev_date]['cloud_percentages'][i] * data[prev_date]['percent_areas'][i]\n",
    "                sum_area_percents += data[prev_date]['percent_areas'][i]\n",
    "\n",
    "            for i in range(len(data[date]['tile_ids'])):\n",
    "                weighted_sum += data[date]['cloud_percentages'][i] * data[date]['percent_areas'][i]\n",
    "                sum_area_percents += data[date]['percent_areas'][i]\n",
    "\n",
    "            avg_cloud_percent = weighted_sum / sum_area_percents\n",
    "            if avg_cloud_percent > cloud_threshold:\n",
    "                prev_date = date\n",
    "                continue\n",
    "            if days_diff >= 5:\n",
    "                all_pids[str(prev_date)] = data[prev_date]['pids']\n",
    "            elif days_diff < 5:\n",
    "                all_pids[str(date)] = data[prev_date]['pids'] + data[date]['pids']\n",
    "                skip_one = True\n",
    "            prev_date = date\n",
    "    final_pids = data_difference_days(all_pids, data_days_interval)\n",
    "    return final_pids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5fc9190a-545a-47f3-9628-47105e5efa1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_s3_bucket = 'sentinel-cogs'\n",
    "source_s3_dir = 'sentinel-s2-l2a-cogs'\n",
    "def pid_to_path(prod_id, band):\n",
    "    lst = prod_id.split('_')\n",
    "    tile = lst[1]\n",
    "    utm_zone, lat_band, grid_sq = str(tile)[:2], str(tile)[2], str(tile)[3:5]\n",
    "    _year, _month, _day = lst[2][0:4], lst[2][4:6], lst[2][6:8]\n",
    "    vsi_ext = '/vsis3/'\n",
    "    band_tif = os.path.join(vsi_ext, source_s3_bucket, source_s3_dir, str(utm_zone), str(lat_band),\n",
    "                            str(grid_sq),str(_year), str(int(_month)), str(prod_id), str(band) + '.tif')\n",
    "    band_tif = band_tif.replace('\\\\', '/')\n",
    "    return band_tif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e638f0d8-51d7-4f5e-8657-6b939ef8b13b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def raster_to_array(raster_file):\n",
    "    ds = gdal.Open(raster_file)\n",
    "    band = ds.GetRasterBand(1)\n",
    "    arr = band.ReadAsArray()\n",
    "    return arr\n",
    "def get_ndvi(red, nir):\n",
    "    arr = (nir - red) / (nir + red)\n",
    "    arr[arr > 1] = 0\n",
    "    arr[arr < -1] = 0\n",
    "    return arr\n",
    "def get_savi(red, nir, l): ## l = soil brightness correction factor could range from (0 -1)\n",
    "    arr = (1.0 + l)*(nir - red) / (nir + red + l)\n",
    "    return arr\n",
    "\n",
    "def get_lswi(narrow_nir, swir):\n",
    "    arr = (narrow_nir - swir) / (narrow_nir + swir)\n",
    "    arr[arr > 1] = 0\n",
    "    arr[arr < -1] = 0\n",
    "    return arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b8d6ca4b-775e-42a3-8d73-cc4a11f80011",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clip_raster(raster_file, output_file=None, shp_file=None, bbox=None, out_width=None, out_height=None,\n",
    "                      return_file=True):\n",
    "#     ds_lst = list()\n",
    "    ds = gdal.BuildVRT('', raster_file, VRTNodata=0, srcNodata=0)\n",
    "    ds1 = gdal.Warp(output_file, ds, format='GTiff', dstNodata=0,\n",
    "              dstSRS=\"+proj=longlat +datum=WGS84 +no_defs +ellps=WGS84 +towgs84=0,0,0\",\n",
    "              cutlineDSName=shp_file, cropToCutline=True)\n",
    "    arr = ds1.ReadAsArray()\n",
    "    arr = arr.astype('int16')\n",
    "\n",
    "def write_raster(ref_raster, array, dst_filename, gdal_GDT_datatype):\n",
    "    driver = gdal.GetDriverByName('GTiff')\n",
    "    cols = array.shape[1]\n",
    "    rows = array.shape[0]\n",
    "    out_ds = driver.Create(dst_filename, cols, rows, 1, gdal_GDT_datatype)\n",
    "    out_ds.GetRasterBand(1).WriteArray(array)\n",
    "\n",
    "    # adding GeoTransform and Projection\n",
    "    data0 = gdal.Open(ref_raster)\n",
    "    geo_trans = data0.GetGeoTransform()\n",
    "    proj = data0.GetProjection()\n",
    "    del data0\n",
    "    out_ds.SetGeoTransform(geo_trans)\n",
    "    out_ds.SetProjection(proj)\n",
    "    out_ds.FlushCache()\n",
    "    del out_ds\n",
    "    return dst_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d202bf74-cd06-4804-9637-84ca555e54f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# s2_tile = '/home/ec2-user/SageMaker/sumit/crop-classification/unsupcc/satellite/satellite_tiles/s2_tile.shp'\n",
    "# shape_file = 'data/farm_data/Farmdata_Jaleubari_Ladhasar_Ratangarh_Churu_1-polygon.shp'\n",
    "# pids = get_product_ids('2021-10-01', '2022-03-30', 10, 5, \n",
    "#                        shape_file=shape_file, bbox=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eb193ad7-7f14-4b3d-b007-3f275cede7cd",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pids' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key, val \u001b[38;5;129;01min\u001b[39;00m \u001b[43mpids\u001b[49m\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m      2\u001b[0m     pid \u001b[38;5;241m=\u001b[39m val[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m      3\u001b[0m     date \u001b[38;5;241m=\u001b[39m pid\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m2\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pids' is not defined"
     ]
    }
   ],
   "source": [
    "for key, val in pids.items():\n",
    "    pid = val[0]\n",
    "    date = pid.split('_')[2]\n",
    "    f_path = os.path.join('data', 'indice', str(date))\n",
    "    try:\n",
    "        os.makedirs(f_path)\n",
    "    except OSError as exc:\n",
    "        if exc.errno != errno.EEXIST:\n",
    "            raise\n",
    "        pass\n",
    "    b4_path = pid_to_path(pid, 'B04')\n",
    "    b8_path = pid_to_path(pid, 'B08')\n",
    "    b8a_path = pid_to_path(pid, 'B8A')\n",
    "    b11_path = pid_to_path(pid, 'B11')\n",
    "    \n",
    "    b4 = clip_raster(raster_file=b4_path, output_file='b4.tif', shp_file='data/farm_data/Farmdata_Jaleubari_Ladhasar_Ratangarh_Churu_1-polygon.shp')\n",
    "    b8 = clip_raster(raster_file=b8_path, output_file='b8.tif', shp_file='data/farm_data/Farmdata_Jaleubari_Ladhasar_Ratangarh_Churu_1-polygon.shp')\n",
    "    \n",
    "    ndvi = get_ndvi(b4,b8)\n",
    "    ndvi = np.around(ndvi, decimals=2, out=None)\n",
    "    write_raster('b4.tif', ndvi, f'{f_path}/ndvi.tif', gdal.GDT_Float32)\n",
    "    \n",
    "    savi = get_savi(b4, b8, 0.428)\n",
    "    savi = np.around(savi, decimals=2, out=None)\n",
    "    write_raster('b4.tif', savi, f'{f_path}/savi.tif', gdal.GDT_Float32)\n",
    "    b4 = b8 = ndvi = savi = None\n",
    "    \n",
    "    b8a = clip_raster(raster_file=b8a_path, output_file='b8a.tif', shp_file='data/farm_data/Farmdata_Jaleubari_Ladhasar_Ratangarh_Churu_1-polygon.shp')\n",
    "    b11 = clip_raster(raster_file=b11_path, output_file='b11.tif', shp_file='data/farm_data/Farmdata_Jaleubari_Ladhasar_Ratangarh_Churu_1-polygon.shp')\n",
    "    \n",
    "    lswi = get_lswi(b8a, b11)\n",
    "    lswi = np.around(lswi, decimals=2, out=None)\n",
    "    write_raster('b8a.tif', lswi, f'{f_path}/lswi.tif', gdal.GDT_Float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3fb67d6-7a46-4249-b54e-929998d166e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "geodf = gpd.read_file(\"data/farm_data/Farmdata_Jaleubari_Ladhasar_Ratangarh_Churu_1-polygon.shp\")\n",
    "date_dir = glob.glob('data/indice/*')\n",
    "date_dir = sorted(date_dir)\n",
    "randomlist = random.sample(range(0, len(geodf)), 10)\n",
    "print(randomlist)\n",
    "for i in randomlist:\n",
    "    dd = defaultdict(list)\n",
    "    f_path = f'indice_ts_data'\n",
    "    try:\n",
    "        os.makedirs(f_path)\n",
    "    except OSError as exc:\n",
    "        if exc.errno != errno.EEXIST:\n",
    "            raise\n",
    "        pass\n",
    "    for _date in date_dir:\n",
    "        data_date = str(_date.split('/')[-1])\n",
    "        dd['date'].append(data_date)\n",
    "        for indice in ['ndvi', 'savi', 'lswi']:\n",
    "            lst = zonal_stats(geodf[i:i+1], f'{_date}/{indice}.tif', stats=\"mean majority\")\n",
    "            dd[f'{indice}_mean'].append(lst[0]['mean'])\n",
    "            dd[f'{indice}_majority'].append(lst[0]['majority'])\n",
    "    df = pd.DataFrame(dd)\n",
    "    df['Khasra_no'] = geodf['Khasra_no'].iloc[i]\n",
    "    df['geometry'] = geodf['geometry'].iloc[i]\n",
    "    df = df.round(3)\n",
    "    df = df[['Khasra_no', 'date', 'ndvi_mean', 'ndvi_majority', 'savi_mean', 'savi_majority','lswi_mean', 'lswi_majority', 'geometry']]\n",
    "    df.to_csv(f_path + f'/farm{i}.csv', index=False)\n",
    "#     gdf = gpd.GeoDataFrame(df, geometry=geometry)\n",
    "#     gdf.to_file(f_path + f'/farm{i}.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a520bc5-62c6-41d4-938f-a949246b41f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "drs = glob.glob('indice_ts_data/*')\n",
    "for item in drs:\n",
    "    df = pd.read_csv(item)\n",
    "    print(item, df.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e4f629d-481c-48bb-88c2-0cc3de427a5e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ba24b9f-6be1-4bc6-9e46-7f40618b2611",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 1\n",
    "import rasterio\n",
    "from rasterio.mask import mask\n",
    "import geopandas as gpd\n",
    "shapefile = gpd.read_file(\"data/farm_data/Farmdata_Jaleubari_Ladhasar_Ratangarh_Churu_1-polygon.shp\")\n",
    "# extract the geometry in GeoJSON format\n",
    "geoms = shapefile.geometry.values # list of shapely geometries\n",
    "geometry = geoms[i] # shapely geometry\n",
    "# transform to GeJSON format\n",
    "from shapely.geometry import mapping\n",
    "geoms = [mapping(geoms[i])]\n",
    "# extract the raster values values within the polygon \n",
    "with rasterio.open(\"data/indice/20211218/ndvi.tif\") as src:\n",
    "     out_image, out_transform = mask(src, geoms, crop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "967a2a8b-93d3-47f1-8b0d-af68e895faff",
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = out_image[0]\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "plt.imshow(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a96a9090-1a97-45fc-bc16-ed05711bb12d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b059d5b4-482d-4d59-8af5-9e353bdac2b2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
